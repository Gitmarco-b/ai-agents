version: '3.8'

services:
  # Trading app WITH Ollama local AI models
  # Use this if you want to run local AI models (requires more resources)
  trading-app-ollama:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: moon-dev-trading-ollama
    ports:
      - "5000:5000"    # Dashboard
      - "11434:11434"  # Ollama API
    volumes:
      - ./src/data:/app/src/data        # Persist agent data
      - ./temp_data:/app/temp_data      # Persist temp analysis
      - ollama-models:/root/.ollama     # Persist Ollama models
    env_file:
      - .env
    restart: unless-stopped
    # Resource limits (adjust based on your system)
    deploy:
      resources:
        limits:
          memory: 8G  # Ollama needs significant RAM for large models
        reservations:
          memory: 4G
    # Recommended: Use GPU if available
    # runtime: nvidia
    # environment:
    #   - NVIDIA_VISIBLE_DEVICES=all

  # Trading app WITHOUT Ollama (cloud AI only)
  # Use this for lightweight deployment with cloud AI providers
  trading-app:
    build:
      context: .
      dockerfile: Dockerfile.no-ollama
    container_name: moon-dev-trading
    ports:
      - "5000:5000"    # Dashboard
    volumes:
      - ./src/data:/app/src/data        # Persist agent data
      - ./temp_data:/app/temp_data      # Persist temp analysis
    env_file:
      - .env
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 2G
        reservations:
          memory: 1G

volumes:
  ollama-models:
    driver: local

# Usage:
# -------
# With Ollama (local AI models):
#   docker-compose up trading-app-ollama
#
# Without Ollama (cloud AI only):
#   docker-compose up trading-app
#
# Build and run:
#   docker-compose up --build trading-app-ollama
#   docker-compose up --build trading-app
#
# Run in background:
#   docker-compose up -d trading-app-ollama
#
# View logs:
#   docker-compose logs -f trading-app-ollama
#
# Stop:
#   docker-compose down
